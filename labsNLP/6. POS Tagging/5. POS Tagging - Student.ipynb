{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagsets\n",
    "The collection of tags used for a particular task \n",
    "Tags for parts of speech:\t\n",
    "- Nouns, verbs, adverbs, adjectives, articles, etc\t\n",
    "- Subtagging\n",
    "    - nouns\tcan\tbe singular\tor plural\t\n",
    "    - verbs\thave tenses\t\n",
    "- Different\ttagsets\thave different focuses\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Tagger\n",
    "The tagger converts a sentence, in the form of a list of words,\n",
    "into a list of tuples, where each tuple is of the form (word, tag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don’t even remember what all the tags mean sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homographs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\") \n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"refuse\" and \"permit\" both appear as a present tense verb (VBP) and a noun (NN). E.g. refUSE is a verb meaning \"deny,\" while REFuse is a noun meaning \"trash\" (i.e. they are not homophones). \n",
    "Thus, we need to know which word is being used in order to pronounce the text correctly. (For this reason, text-to-speech systems usually perform POS-tagging.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagged Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Tagged Tokens\n",
    "By convention in NLTK, a tagged token is represented using a tuple consisting of the token and the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fly', 'NN')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
    "tagged_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_token[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a list of tagged tokens directly from a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = '''\n",
    "The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN\n",
    "other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC\n",
    "Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS\n",
    "said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB\n",
    "accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT\n",
    "interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n",
    "'''\n",
    "# TODO: Convert it to list of tuples (word,tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Tagged Corpora\n",
    "Several of the corpora included with NLTK have been tagged for their part-of-speech. Here's an example of what you might see if you opened a file from the Brown Corpus with a text editor:\n",
    "\n",
    "*The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd / no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many Tag Sets\n",
    "Different corpora have different conventions for tagging.\t\n",
    "- NLTK made a simplified, unified tagset\n",
    " …\twhich\tno\tone\tuses.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.brown.tagged_words())\n",
    "print(nltk.corpus.brown.tagged_words(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]\n",
      "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.treebank.tagged_words())\n",
    "print(nltk.corpus.treebank.tagged_words(tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exploring Tagged Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Tagset\n",
    "Let's find the list of universal tages sorted by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown \n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal') \n",
    "# TODO: do it here\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's find words involving particular sequences of tags and words (in this case \"<Verb> to <Verb>\"). \n",
    "    In code-three-word-phrase we consider each three-word window in the sentence, and check if they meet our criterion. If the tags match, we print the corresponding words.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined to achieve\n",
      "continue to place\n",
      "serve to protect\n",
      "wanted to wait\n",
      "allowed to place\n",
      "expected to become\n",
      "expected to approve\n",
      "expected to make\n",
      "intends to make\n",
      "seek to set\n",
      "like to see\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "def process(sentence): \n",
    "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence):\n",
    "        if (t1.startswith('V') and t2 == 'TO' and t3.startswith('V')):\n",
    "            print(w1, w2, w3)\n",
    "            \n",
    "for tagged_sent in brown.tagged_sents()[:100]:\n",
    "    process(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Syntax note: Python Default Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Tagging\n",
    "We will explore various ways to automatically add part-of-speech tags to text. We will see that the tag of a word depends on the word and its context within a sentence. For this reason, we will be working with data at the level of (tagged) sentences rather than words. We'll begin by loading the data we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Default Tagger\n",
    "The simplest possible tagger assigns the same tag to each token. This may seem to be a rather banal step, but it establishes an important baseline for tagger performance. In order to get the best result, we tag each word with the most likely tag. Let's find out which tag is most likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_tagged_words = brown.tagged_words(categories='news')\n",
    "#TODO: what is the most frequent tag in brown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a tagger that tags everything as NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'NN'), ('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('green', 'NN'), ('eggs', 'NN'), ('and', 'NN'), ('ham', 'NN'), (',', 'NN'), ('I', 'NN'), ('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('them', 'NN'), ('Sam', 'NN'), ('I', 'NN'), ('am', 'NN'), ('!', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "print(default_tagger.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, this method performs rather **poorly**. On a typical corpus, it will tag only about an eighth of the tokens correctly, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How to evalute the porformance?\n",
    "We evaluate the performance of a tagger relative to the tags a human expert would assign. Since we don't usually have access to an expert and impartial human judge, we make do instead with gold standard test data. This is a corpus which has been manually annotated and which is accepted as a standard against which the guesses of an automatic system are assessed. The tagger is regarded as being correct if the tag it guesses for a given word is the same as the gold standard tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13089484257215028"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Regular Expression Tagger\n",
    "The regular expression tagger assigns tags to tokens on the basis of human-defined matching patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "     (r'.*ing$', 'VBG'),               # gerunds\n",
    "     (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'.*', 'NN')                     # nouns (default)\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', 'NN'), ('Only', 'NN'), ('a', 'NN'), ('relative', 'NN'), ('handful', 'NN'), ('of', 'NN'), ('such', 'NN'), ('reports', 'NNS'), ('was', 'NNS'), ('received', 'VBD'), (\"''\", 'NN'), (',', 'NN'), ('the', 'NN'), ('jury', 'NN'), ('said', 'NN'), (',', 'NN'), ('``', 'NN'), ('considering', 'VBG'), ('the', 'NN'), ('widespread', 'NN'), ('interest', 'NN'), ('in', 'NN'), ('the', 'NN'), ('election', 'NN'), (',', 'NN'), ('the', 'NN'), ('number', 'NN'), ('of', 'NN'), ('voters', 'NNS'), ('and', 'NN'), ('the', 'NN'), ('size', 'NN'), ('of', 'NN'), ('this', 'NNS'), ('city', 'NN'), (\"''\", 'NN'), ('.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "print(regexp_tagger.tag(brown_sents[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20326391789486245"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Lookup Tagger (Unigram Tagger)\n",
    "A lot of high-frequency words do not have the NN tag. Let's find the hundred most frequent words and store their most likely tag. We can then use this information as the model for a \"lookup tagger\" (an NLTK UnigramTagger):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-adc7bf950e00>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-adc7bf950e00>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    likely_tags =\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fd = nltk.FreqDist(brown.words(categories='news'))\n",
    "cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "\n",
    "most_freq_words = fd.most_common(100)\n",
    "\n",
    "# TODO : make a dictionary of (word, most common tag for this word) for each word in most_freq_words\n",
    "likely_tags = \n",
    "\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "\n",
    "baseline_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should come as no surprise by now that simply knowing the tags for the 100 most frequent words enables us to tag a large fraction of tokens correctly (nearly half in fact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = brown.sents(categories='news')[3]\n",
    "#TODO use it to tag sent\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Many words have been assigned a tag of None, because they were not among the 100 most frequent words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combining taggers with backoff tagging\n",
    "**Backoff tagging**: It allows you to chain taggers together so that if one tagger doesn't know how to tag a word, it can pass the word on to the next backoff tagger. If that one can't do it, it can pass the word on to the next backoff tagger, and so on until there are no backoff taggers left to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags,backoff=nltk.DefaultTagger('NN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all this together and write a program to create and evaluate lookup taggers having a range of sizes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(cfd, wordlist):\n",
    "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
    "    return baseline_tagger.evaluate(brown.tagged_sents(categories='news'))\n",
    "\n",
    "def display():\n",
    "    import pylab\n",
    "    word_freqs = nltk.FreqDist(brown.words(categories='news')).most_common()\n",
    "    words_by_freq = [w for (w, _) in word_freqs]\n",
    "    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "    sizes = 2 ** pylab.arange(15)\n",
    "    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
    "    pylab.plot(sizes, perfs, '-bo')\n",
    "    pylab.title('Lookup Tagger Performance with Varying Model Size')\n",
    "    pylab.xlabel('Model Size')\n",
    "    pylab.ylabel('Performance')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  N-Gram Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Tagging\n",
    "Unigram taggers are based on a simple statistical algorithm: for each token, assign the tag that is most likely for that particular token.\n",
    "A unigram tagger behaves just like a lookup tagger, except there is a more convenient technique for setting it up, called training. In the following code sample, we train a unigram tagger, use it to tag a sentence, then evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_sents = brown.sents(categories='news')\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "# TODO: make a unigram tagger using brown_tagged_sents as a training data, simply be passing it to the UnigramTagger\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "\n",
    "print(unigram_tagger.tag(brown_sents[2007]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tagger.evaluate(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Separating the Training and Testing Data\n",
    "A tagger that simply memorized its training data and made no attempt to construct a general model would get a perfect score, but would also be useless for tagging new text. Instead, we should split the data, training on 90% and testing on the remaining 10%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "print(size)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other data: treebank\n",
    "#from nltk.corpus import treebank\n",
    "#train_sents = treebank.tagged_sents()[:3000]\n",
    "#test_sents = treebank.tagged_sents()[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General N-Gram Tagging\n",
    "An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the n-1 preceding tokens, \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/tag-context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "print(bigram_tagger.tag(brown_sents[2007]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_sent = brown_sents[4203]\n",
    "print(bigram_tagger.tag(unseen_sent))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notice that the bigram tagger manages to tag every word in a sentence it saw during training, but does badly on an unseen sentence.\n",
    "Let's Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Evaluate bigram_tagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "*sparse data problem* As n gets larger, the specificity of the contexts increases, as does the chance that the data we wish to tag contains contexts that were not present in the training data. \n",
    " there is a trade-off between the accuracy and the coverage of our results (and this is related to the *precision/recall trade-off* in information retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layering taggers\n",
    "Use the benefits of several types of taggers\t\n",
    "- Try the bigram tagger\t\n",
    "- When it is unable to find a tag, use the unigram tagger\t\n",
    "- If that fails, then use the default tagger\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b929f90747e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDefaultTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnigramTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#TODO: build a bigram tagger that backs off to unigram tagger t1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sents' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "#TODO: build a bigram tagger that backs off to unigram tagger t1\n",
    "\n",
    "t2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all togather in a method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-41db893d07fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDefaultTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnigramTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBigramTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdefault_tagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDefaultTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minitial_tagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackoff_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mUnigramTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBigramTagger\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_tagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0minitial_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sents' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger\n",
    "default_tagger = DefaultTagger('NN')\n",
    "initial_tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger], backoff=default_tagger)\n",
    "initial_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tagging Unknown Words\n",
    "- Our approach to tagging unknown words still uses *backoff* to a regular-expression tagger or a default tagger. These are unable to make use of context.\n",
    "- if our tagger encountered the word blog, not seen during training, it would assign it the same tag, regardless of whether this word appeared in the context *the blog* or *to blog*\n",
    "- How can we do better with these unknown words, or **out-of-vocabulary** items?\n",
    "limit the vocabulary of a tagger to the most frequent n words, and to replace every other word with a special word *UNK* During training, a unigram tagger will probably learn that *UNK* is usually a *noun*.\n",
    "However, the n-gram taggers will detect contexts in which it has some other tag. For example, if the preceding word is to (tagged TO), then *UNK* will probably be tagged as a *verb*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Storing Taggers\n",
    "Training a tagger on a large corpus may take a significant time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-df2e124b461b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't2.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 't2' is not defined"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "output = open('t2.pkl', 'wb')\n",
    "dump(t2, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-21533198ce30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't2.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "input = open('t2.pkl', 'rb')\n",
    "tagger = load(input)\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-7ac0ae2cecbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     is up against in our complex maze of regulatory laws .\"\"\"\n\u001b[0;32m      3\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tagger' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The board's action shows what free enterprise\n",
    "...     is up against in our complex maze of regulatory laws .\"\"\"\n",
    "tokens = text.split()\n",
    "print(tagger.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance Limitations\n",
    "What is the upper limit to the performance of an n-gram tagger? \n",
    "- What is the precentage of words that have ambiguous contexts and could be assigned the wrong tag ...\n",
    "Using trigram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "            ((x[1], y[1], z[0]), z[1])\n",
    "            for sent in brown_tagged_sents\n",
    "            for x, y, z in nltk.trigrams(sent))\n",
    "ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
    "sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **confusion matrix** to study the tagger's mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How *\"fine\"* the tagset should be?**\n",
    "tagging process collapses distinctions: e.g. *lexical identity* is usually lost when all personal pronouns are tagged *PRP*. At the same time, the tagging process introduces new *distinction*s and removes ambiguities: e.g. *deal* tagged as *VB* or *NN*. \n",
    "- Finer distinctions (detailed tagset):  more information to tag on, but more work to do\n",
    "- Fewer distinctions ( simplified tagset):  less information about context, but smaller range of choices in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What’s wrong with this picture? \n",
    "- Size of the n-gram table language model\n",
    "- Context!\t\n",
    "        Words,not just pos, matter in context\t\n",
    "- Not understandable rules\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Transformation-Based Tagging (Brill Tagger)\n",
    "Brill tagging is a kind of transformation-based learning, named after its inventor.\n",
    "- The general idea: guess the tag of each word, then go back and fix the mistakes. \n",
    "- It uses supervised learning to build a list of transformational correction rules.\n",
    "- First tag with the unigram tagger, then applying the rules to fix the errors.\n",
    "- Rules (\"replace T1 with T2 in the context C\") \n",
    "    like:\n",
    "    - (a) Replace *NN* with *VB* when the *previous* **word** is *TO*; \n",
    "    - (b) Replace *TO* with *IN* when the *next* **tag** is *NNS*\n",
    "- Training: \n",
    "    - **guess** values for T1, T2 and C, to create thousands of candidate rules.\n",
    "    - Each rule is **scored** according to its net benefit: the number of incorrect tags that it corrects, less the number of correct tags it incorrectly modifies\n",
    "- The rules are linguistically **interpretable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tagged data from treebank... \n",
      "Read testing data (200 sents/5251 wds)\n",
      "Read training data (800 sents/19933 wds)\n",
      "Read baseline data (800 sents/19933 wds) [reused the training set]\n",
      "Trained baseline tagger\n",
      "    Accuracy on test set: 0.8366\n",
      "Training tbl tagger...\n",
      "TBL train (fast) (seqs: 800; tokens: 19933; tpls: 24; min score: 3; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 12799 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      "  23  23   0   0  | POS->VBZ if Pos:PRP@[-2,-1]\n",
      "  18  19   1   0  | NN->VB if Pos:-NONE-@[-2] & Pos:TO@[-1]\n",
      "  14  14   0   0  | VBP->VB if Pos:MD@[-2,-1]\n",
      "  12  12   0   0  | VBP->VB if Pos:TO@[-1]\n",
      "  11  11   0   0  | VBD->VBN if Pos:VBD@[-1]\n",
      "  11  11   0   0  | IN->WDT if Pos:-NONE-@[1] & Pos:VBP@[2]\n",
      "  10  11   1   0  | VBN->VBD if Pos:PRP@[-1]\n",
      "   9  10   1   0  | VBD->VBN if Pos:VBZ@[-1]\n",
      "   8   8   0   0  | NN->VB if Pos:MD@[-1]\n",
      "   7   7   0   1  | VB->NN if Pos:DT@[-1]\n",
      "   7   7   0   0  | VB->VBP if Pos:PRP@[-1]\n",
      "   7   7   0   0  | IN->WDT if Pos:-NONE-@[1] & Pos:VBZ@[2]\n",
      "   7   8   1   0  | IN->RB if Word:as@[2]\n",
      "   6   6   0   0  | VBD->VBN if Pos:VBP@[-2,-1]\n",
      "   6   6   0   1  | IN->WDT if Pos:-NONE-@[1] & Pos:VBD@[2]\n",
      "   5   5   0   0  | POS->VBZ if Pos:-NONE-@[-1]\n",
      "   5   5   0   0  | VB->VBP if Pos:NNS@[-1]\n",
      "   5   5   0   0  | VBD->VBN if Word:be@[-2,-1]\n",
      "   4   4   0   0  | POS->VBZ if Pos:``@[-2]\n",
      "   4   4   0   0  | VBP->VB if Pos:VBD@[-2,-1]\n",
      "   4   6   2   3  | RP->RB if Pos:CD@[1,2]\n",
      "   4   4   0   0  | RB->JJ if Pos:DT@[-1] & Pos:NN@[1]\n",
      "   4   4   0   0  | NN->VBP if Pos:NNS@[-2] & Pos:RB@[-1]\n",
      "   4   5   1   0  | VBN->VBD if Pos:NNP@[-2] & Pos:NNP@[-1]\n",
      "   4   4   0   0  | IN->WDT if Pos:-NONE-@[1] & Pos:MD@[2]\n",
      "   4   8   4   0  | VBD->VBN if Word:*@[1]\n",
      "   4   4   0   0  | JJS->RBS if Word:most@[0] & Word:the@[-1] & Pos:DT@[-1]\n",
      "   3   3   0   0  | VBD->VBN if Pos:VBN@[-1]\n",
      "   3   4   1   0  | VBN->VB if Pos:TO@[-1]\n",
      "   3   4   1   1  | IN->RB if Pos:.@[1]\n",
      "   3   3   0   0  | JJ->RB if Pos:VBD@[1]\n",
      "   3   3   0   0  | PRP$->PRP if Pos:TO@[1]\n",
      "   3   3   0   0  | NN->VBP if Pos:NNS@[-1] & Pos:DT@[1]\n",
      "   3   3   0   0  | VBP->VB if Word:n't@[-2,-1]\n",
      "Trained tbl tagger in 4.06 seconds\n",
      "    Accuracy on test set: 0.8572\n",
      "Tagging the test data\n"
     ]
    }
   ],
   "source": [
    "from nltk.tbl import demo as brill_tagger\n",
    "brill_tagger.demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Brill Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import brill, brill_trainer\n",
    "def train_brill_tagger(initial_tagger, train_sents, **kwargs):\n",
    "    templates = [\n",
    "        brill.Template(brill.Pos([-1])),\n",
    "        brill.Template(brill.Pos([1])),\n",
    "        brill.Template(brill.Pos([-2])),\n",
    "        brill.Template(brill.Pos([2])),\n",
    "        brill.Template(brill.Pos([-2, -1])),\n",
    "        brill.Template(brill.Pos([1, 2])),\n",
    "        brill.Template(brill.Pos([-3, -2, -1])),\n",
    "        brill.Template(brill.Pos([1, 2, 3])),\n",
    "        brill.Template(brill.Pos([-1]), brill.Pos([1])),\n",
    "        brill.Template(brill.Word([-1])),\n",
    "        brill.Template(brill.Word([1])),brill.Template(brill.Word([-2])),\n",
    "        brill.Template(brill.Word([2])),\n",
    "        brill.Template(brill.Word([-2, -1])),\n",
    "        brill.Template(brill.Word([1, 2])),\n",
    "        brill.Template(brill.Word([-3, -2, -1])),\n",
    "        brill.Template(brill.Word([1, 2, 3])),\n",
    "        brill.Template(brill.Word([-1]), brill.Word([1])),\n",
    "    ]\n",
    "    trainer = brill_trainer.BrillTaggerTrainer(initial_tagger,templates, deterministic=True)\n",
    "    return trainer.train(train_sents, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compare Trigram tagger with brill***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-0cd9a0d82450>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrigramTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minitial_tagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackoff_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mUnigramTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBigramTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrigramTagger\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_tagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0minitial_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sents' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tag import TrigramTagger\n",
    "initial_tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=default_tagger)\n",
    "initial_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brill_tagger = train_brill_tagger(initial_tagger, train_sents)\n",
    "brill_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the BrillTagger has slightly increased accuracy over the initial_tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taggers are just classifiers\n",
    "The modern approach: just turn your training data into features and throw them into a good classifier\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TnT tagger\n",
    "TnT stands for Trigrams'n'Tags. It is a statistical tagger based on second order Markov models.\n",
    "The TnT tagger maintains a number of internal FreqDist and ConditionalFreqDist instances based on the training data. \n",
    "- These frequency distributions *count unigrams, bigrams, and trigrams*. \n",
    "- Then, during tagging, the frequencies are used to calculate the probabilities of possible tags for each word. So, instead of constructing a backoff chain of NgramTagger subclasses, the TnT tagger **uses all the ngram models together to choose the best tag**. \n",
    "- It also tries to guess the tags for the **whole sentence** at once by choosing the most likely model for the entire sentence, based on the probabilities of each possible tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import tnt\n",
    "tnt_tagger = tnt.TnT()\n",
    "tnt_tagger.train(train_sents)\n",
    "tnt_tagger.evaluate(test_sents[:100])\n",
    "#tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is fast but Tagging Process is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier-based tagging\n",
    "The ClassifierBasedPOSTagger class uses classification to do part-of-speech tagging. Features are extracted from words, and then passed  to an internal classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It defaults to training a ***NaiveBayesClassifier*** class with the given training data.\n",
    "The feature detector finds multiple length suffixes, does some regular expression matching, and looks at the unigram, bigram, and trigram history to produce a fairly complete set of features for each word.\n",
    "The feature sets it produces are used to train the internal classifier, and are used for classifying words into part-of-speech tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "tagger = ClassifierBasedPOSTagger(train=train_sents)\n",
    "tagger.evaluate(test_sents)\n",
    "#0.9309734513274336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ***MaxentClassifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "me_tagger = ClassifierBasedPOSTagger(train=train_sents, classifier_builder=MaxentClassifier.train)\n",
    "me_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting features with a custom feature detector\n",
    "If you want to do your own feature detection, there are two ways to do it:\n",
    "1. Subclass ClassifierBasedTagger and implement a feature_detector() method.\n",
    "2. Pass a function as the feature_detector keyword argument into ClassifierBasedTagger at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_feature_detector(tokens, index, history):\n",
    "    return {'word': tokens[index]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedTagger\n",
    "\n",
    "tagger = ClassifierBasedTagger(train=train_sents, feature_detector=unigram_feature_detector)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Determine the Category of a Word\n",
    "- Morphological Clues: The internal structure of a word may give useful clues as to the word's category. \n",
    "    For example, -ness is a suffix that combines with an adjective to produce a noun, e.g. happy → happiness.\n",
    "- Syntactic Clues: the typical contexts in which a word can occur. \n",
    "    For example, adjective in English is that it can occur immediately before a noun, or immediately following the words *be* or *very*. e.g. the near window .... The end is (very) near.\n",
    "- Semantic Clues: the meaning of a word.  \n",
    "    For example, the best-known definition of a noun is semantic: \"the name of a person, place or thing\".\n",
    "\n",
    "New Words: mostly nouns, (open class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
