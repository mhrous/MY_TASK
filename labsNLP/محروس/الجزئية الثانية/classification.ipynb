{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import  RidgeClassifier,LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# مشروحين بالجزئية الاولى"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLinks(text):\n",
    "    LINK_RS = r'http(s)?\\S+'\n",
    "    PIC_LINK_RS = r'pic\\S+'\n",
    "    result = re.sub(LINK_RS,' ', text)\n",
    "    result = re.sub(PIC_LINK_RS,' ', result)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# مشروحين بالجزئية الاولى "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    " def normalizeLetters(text):\n",
    "    #لاازالة الحروف المتتكرة في مشكلة انو اذا كانو حربين ولا بعض نفس الشي بيساويهون واحد مثلا https ->htps\n",
    "    result = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    #تصليح ههه\n",
    "    #في مشكلة لما تكون هههه باخر الجملة\n",
    "    result = re.sub(' ه(\\s|$)',\" ههه \" ,result)\n",
    "    result = re.sub(r\"[@,;/(){}\\[\\]\\|]\", \" \", result)\n",
    "    result = re.sub(\"[أآ]\", \"ا\", result)\n",
    "\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtags(text): \n",
    "    text=re.findall(r\"#[\\w\\d_]+\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remov_stopwords(sentence):\n",
    "    stopword = nltk.corpus.stopwords.words('arabic')\n",
    "    words =[ word for word in  nltk.word_tokenize(sentence) if word not in stopword]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    text=re.sub(r\"#\",' ',text)\n",
    "    text=re.sub(r\"(\\w+)_(\\w+)\",' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# تحويل الهاش تاغ الى سترينغ عادية "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_hashtags_to_string(text):\n",
    "    text=re.sub(r\"#|_\",r' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# معالجة النص "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data):\n",
    "    def sentence_preprocess(sentencs):\n",
    "        res = removeLinks(sentencs)\n",
    "        res = normalizeLetters(res)\n",
    "        res = remov_stopwords(res)\n",
    "        return res\n",
    "    array =[sentence_preprocess(sent) for sent in data]\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# قرائة ملفات csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv(\"test.csv\",sep=\"\\t\")\n",
    "df_train =pd.read_csv(\"train_without_any_of_test.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# وضع الداتا في قوائم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>NEG</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>5310</td>\n",
       "      <td>4545</td>\n",
       "      <td>9855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>3298</td>\n",
       "      <td>3132</td>\n",
       "      <td>6430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           POS   NEG  Count\n",
       "0  Train  5310  4545   9855\n",
       "1   Test  3298  3132   6430"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_nan = []\n",
    "X_train = df_train[\"tweet\"].tolist()\n",
    "index_nan =[index  for index,val in enumerate(X_train) if str(val) == 'nan'  ]\n",
    "X_train =[x for x in X_train if str(x) != 'nan']\n",
    "X_train =data_preprocess(X_train)\n",
    "\n",
    "y_train = df_train[\"label\"].tolist()\n",
    "for index in index_nan:\n",
    "    y_train.pop(index)\n",
    "y_train =data_preprocess(y_train)\n",
    "\n",
    "\n",
    "index_nan = []\n",
    "X_test = df_test[\"tweet\"].tolist()\n",
    "index_nan =[index  for index,val in enumerate(X_test) if str(val) == 'nan'  ]\n",
    "X_test =[x for x in X_test if str(x) != 'nan']\n",
    "X_test =data_preprocess(X_test)\n",
    "\n",
    "\n",
    "y_test = df_test[\"label\"].tolist()\n",
    "y_test =[x for x in y_test if str(x) != 'nan']\n",
    "for index in index_nan:\n",
    "    y_test.pop(index)\n",
    "y_test =data_preprocess(y_test)\n",
    "\n",
    "\n",
    "df_info = pd.DataFrame([[\"Train\",y_train.count(\"POS\"),y_train.count(\"NEG\"),len(y_train)],\n",
    "                       [\"Test\",y_test.count(\"POS\"),y_test.count(\"NEG\"),len(y_test)]], columns = [\"\",'POS', 'NEG',\"Count\"])\n",
    "df_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bow من اجل تطبيق خورزميات classfiction \n",
    "تحويل النص اللى قيم عددية\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_features(X_train, X_test ,min_ngram=1, max_ngram=1):\n",
    "    \n",
    "    count_vectorizer = CountVectorizer(ngram_range=(min_ngram, max_ngram))\n",
    "    X_train_Vec = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_Vec = count_vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_Vec, X_test_Vec, count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf من اجل تطبيق خورزميات classfiction¶\n",
    "تحويل النص اللى قيم عددية"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_test ,min_ngram=1 ,max_ngram=1):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(min_ngram, max_ngram))\n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تطبيق نوع من خورزميات الماشين لينرنيغ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    classifier = RidgeClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تطبيق خورزميات "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,min_ngram,max_ngram):\n",
    "    tifidf = []\n",
    "    bag = []\n",
    "    max_Accuracy_tifidf =0\n",
    "    max_Accuracy_bag = 0\n",
    "    for _min in range(min_ngram,max_ngram):\n",
    "        for _max in range(_min , max_ngram):           \n",
    "            X_train_bag, X_test_bag, bow_vectorizer = bow_features(X_train, X_test, min_ngram =_min, max_ngram=_max)\n",
    "            X_train_tfidf, X_test_tfidf, tfidf_vectorizer = tfidf_features(X_train, X_test, min_ngram =_min, max_ngram=_max)\n",
    "            classifier_bag = train_classifier(X_train_bag, y_train)\n",
    "            classifier_tfidf = train_classifier(X_train_tfidf, y_train)\n",
    "            \n",
    "            y_test_predicted_labels_bag = classifier_bag.predict(X_test_bag)\n",
    "            y_test_predicted_scores_bag = classifier_bag.decision_function(X_test_bag)\n",
    "           \n",
    "            y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "            y_test_predicted_scores_tfidf = classifier_tfidf.decision_function(X_test_tfidf)\n",
    "            \n",
    "            accuracy_Score_bag = accuracy_score(y_test, y_test_predicted_labels_bag)\n",
    "            accuracy_Score_tfidf = accuracy_score(y_test, y_test_predicted_labels_tfidf)\n",
    "            if(accuracy_Score_bag>max_Accuracy_bag):\n",
    "                max_Accuracy_bag = accuracy_Score_bag\n",
    "                bag =[bow_vectorizer,classifier_bag]\n",
    "                \n",
    "            if(accuracy_Score_tfidf>max_Accuracy_tifidf):\n",
    "                max_Accuracy_tifidf = accuracy_Score_tfidf\n",
    "                tifidf =[tfidf_vectorizer,classifier_tfidf]\n",
    "\n",
    "            \n",
    "            print(f\"min_gram  {_min} , max_gram  {_max}\")\n",
    "            print(f'Bag-of-words Accuracy: {str(accuracy_Score_bag)}')\n",
    "            print(f'Tfidf Accuracy: {str(accuracy_Score_tfidf)}')\n",
    "            print(\"\\n\\n_____________________________________________________________________________\")\n",
    "            \n",
    "            \n",
    "        \n",
    "    return tifidf,bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_gram  1 , max_gram  1\n",
      "Bag-of-words Accuracy: 0.7947122861586314\n",
      "Tfidf Accuracy: 0.8074650077760498\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  2\n",
      "Bag-of-words Accuracy: 0.8082426127527216\n",
      "Tfidf Accuracy: 0.8132192846034214\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  3\n",
      "Bag-of-words Accuracy: 0.8096423017107309\n",
      "Tfidf Accuracy: 0.8124416796267496\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.8074650077760498\n",
      "Tfidf Accuracy: 0.8118195956454122\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.8034214618973562\n",
      "Tfidf Accuracy: 0.812597200622084\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  2\n",
      "Bag-of-words Accuracy: 0.7418351477449455\n",
      "Tfidf Accuracy: 0.7438569206842924\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  3\n",
      "Bag-of-words Accuracy: 0.7377916018662519\n",
      "Tfidf Accuracy: 0.7429237947122862\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.7337480559875583\n",
      "Tfidf Accuracy: 0.7432348367029549\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.730637636080871\n",
      "Tfidf Accuracy: 0.7430793157076205\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  3 , max_gram  3\n",
      "Bag-of-words Accuracy: 0.7062208398133748\n",
      "Tfidf Accuracy: 0.6648522550544324\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  3 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.6990668740279938\n",
      "Tfidf Accuracy: 0.6623639191290824\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  3 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.6961119751166407\n",
      "Tfidf Accuracy: 0.6620528771384137\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  4 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.6875583203732504\n",
      "Tfidf Accuracy: 0.6023328149300156\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  4 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.6824261275272162\n",
      "Tfidf Accuracy: 0.6018662519440124\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  5 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.6754276827371695\n",
      "Tfidf Accuracy: 0.5758942457231726\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tifidf,bag = train(X_train,y_train,X_test,y_test,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your tweet:  اسقاط الولاية \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : اسقاط الولاية \n",
      "tifidf : POS\n",
      "bag :POS\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter your tweet: \")\n",
    "tr_tifidf = tifidf[0].transform([text])\n",
    "pr_tifidf = tifidf[1].predict(tr_tifidf)\n",
    "\n",
    "tr_bag = bag[0].transform([text])\n",
    "pr_bag = bag[1].predict(tr_bag)\n",
    "\n",
    "print(f\"text : {text}\\ntifidf : {pr_tifidf[0]}\\nbag :{pr_bag[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reHash = [remove_hashtags(x) for x in X_train]\n",
    "y_train_reHash = y_train[:]\n",
    "X_test_reHash = [remove_hashtags(x) for x in X_test]\n",
    "y_test_reHash =y_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove hashtags\n",
      "min_gram  1 , max_gram  1\n",
      "Bag-of-words Accuracy: 0.7762052877138413\n",
      "Tfidf Accuracy: 0.7914463452566096\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  2\n",
      "Bag-of-words Accuracy: 0.787402799377916\n",
      "Tfidf Accuracy: 0.7981337480559876\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  3\n",
      "Bag-of-words Accuracy: 0.7869362363919129\n",
      "Tfidf Accuracy: 0.7964230171073094\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.7870917573872472\n",
      "Tfidf Accuracy: 0.7976671850699845\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.7853810264385692\n",
      "Tfidf Accuracy: 0.7979782270606531\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  2\n",
      "Bag-of-words Accuracy: 0.7398133748055987\n",
      "Tfidf Accuracy: 0.7401244167962675\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  3\n",
      "Bag-of-words Accuracy: 0.7354587869362363\n",
      "Tfidf Accuracy: 0.7405909797822706\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.7317262830482115\n",
      "Tfidf Accuracy: 0.7415241057542769\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.728149300155521\n",
      "Tfidf Accuracy: 0.7410575427682737\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  3 , max_gram  3\n",
      "Bag-of-words Accuracy: 0.7027993779160187\n",
      "Tfidf Accuracy: 0.663452566096423\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  3 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.6962674961119751\n",
      "Tfidf Accuracy: 0.6614307931570762\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  3 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.6930015552099533\n",
      "Tfidf Accuracy: 0.6609642301710731\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  4 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.6841368584758942\n",
      "Tfidf Accuracy: 0.6013996889580093\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  4 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.6810264385692069\n",
      "Tfidf Accuracy: 0.6007776049766719\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  5 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.673094867807154\n",
      "Tfidf Accuracy: 0.5749611197511664\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"remove hashtags\")\n",
    "tifidf_re,bag_re =train(X_train_reHash,y_train_reHash,X_test_reHash,y_test_reHash,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your tweet:  اسقاط الولاية\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : اسقاط الولاية\n",
      "tifidf : POS\n",
      "bag :POS\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter your tweet: \")\n",
    "tr_tifidf_re = tifidf_re[0].transform([text])\n",
    "pr_tifidf_re = tifidf_re[1].predict(tr_tifidf_re)\n",
    "\n",
    "tr_bag_re = bag_re[0].transform([text])\n",
    "pr_bag_re = bag_re[1].predict(tr_bag_re)\n",
    "\n",
    "print(f\"text : {text}\\ntifidf : {pr_tifidf_re[0]}\\nbag :{pr_bag_re[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_strHash = [turn_hashtags_to_string(x) for x in X_train]\n",
    "y_train_strHash = y_train[:]\n",
    "X_test_strHash = [turn_hashtags_to_string(x) for x in X_test]\n",
    "y_test_strHash =y_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string hashtags\n",
      "min_gram  1 , max_gram  1\n",
      "Bag-of-words Accuracy: 0.8017107309486781\n",
      "Tfidf Accuracy: 0.8186625194401245\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  2\n",
      "Bag-of-words Accuracy: 0.8192846034214619\n",
      "Tfidf Accuracy: 0.8265940902021773\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  3\n",
      "Bag-of-words Accuracy: 0.8200622083981337\n",
      "Tfidf Accuracy: 0.8279937791601866\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.8209953343701399\n",
      "Tfidf Accuracy: 0.8264385692068429\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  1 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.8203732503888025\n",
      "Tfidf Accuracy: 0.8262830482115086\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  2\n",
      "Bag-of-words Accuracy: 0.7802488335925349\n",
      "Tfidf Accuracy: 0.7889580093312597\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  3\n",
      "Bag-of-words Accuracy: 0.7755832037325039\n",
      "Tfidf Accuracy: 0.7842923794712287\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.7699844479004666\n",
      "Tfidf Accuracy: 0.7813374805598756\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  2 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.7671850699844479\n",
      "Tfidf Accuracy: 0.7808709175738725\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  3 , max_gram  3\n",
      "Bag-of-words Accuracy: 0.7488335925349923\n",
      "Tfidf Accuracy: 0.7286158631415242\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  3 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.743701399688958\n",
      "Tfidf Accuracy: 0.7287713841368585\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  3 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.7399688958009332\n",
      "Tfidf Accuracy: 0.7283048211508554\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  4 , max_gram  4\n",
      "Bag-of-words Accuracy: 0.7273716951788491\n",
      "Tfidf Accuracy: 0.6909797822706065\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  4 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.7245723172628304\n",
      "Tfidf Accuracy: 0.6912908242612753\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n",
      "min_gram  5 , max_gram  5\n",
      "Bag-of-words Accuracy: 0.7062208398133748\n",
      "Tfidf Accuracy: 0.6099533437013996\n",
      "\n",
      "\n",
      "_____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"string hashtags\")\n",
    "tifidf_str,bag_str=train(X_train_strHash,y_train_strHash,X_test_strHash,y_test_strHash,1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your tweet:  سعوديات مع الولاية\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : سعوديات مع الولاية\n",
      "tifidf : POS\n",
      "bag :POS\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter your tweet: \")\n",
    "tr_tifidf_str = tifidf_str[0].transform([text])\n",
    "pr_tifidf_str = tifidf_str[1].predict(tr_tifidf_str)\n",
    "\n",
    "tr_bag_str = bag_str[0].transform([text])\n",
    "pr_bag_str = bag_str[1].predict(tr_bag_str)\n",
    "\n",
    "print(f\"text : {text}\\ntifidf : {pr_tifidf_str[0]}\\nbag :{pr_bag_str[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your tweet:  اسقاط الولية\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : اسقاط الولية\n",
      "tifidf : NEG\n",
      "bag :POS\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter your tweet: \")\n",
    "tr_tifidf_str = tifidf_str[0].transform([text])\n",
    "pr_tifidf_str = tifidf_str[1].predict(tr_tifidf_str)\n",
    "\n",
    "tr_bag_str = bag_str[0].transform([text])\n",
    "pr_bag_str = bag_str[1].predict(tr_bag_str)\n",
    "\n",
    "print(f\"text : {text}\\ntifidf : {pr_tifidf_str[0]}\\nbag :{pr_bag_str[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your tweet:  سحر الجنوب\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : سحر الجنوب\n",
      "tifidf : NEG\n",
      "bag :POS\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter your tweet: \")\n",
    "tr_tifidf_str = tifidf_str[0].transform([text])\n",
    "pr_tifidf_str = tifidf_str[1].predict(tr_tifidf_str)\n",
    "\n",
    "tr_bag_str = bag_str[0].transform([text])\n",
    "pr_bag_str = bag_str[1].predict(tr_bag_str)\n",
    "\n",
    "print(f\"text : {text}\\ntifidf : {pr_tifidf_str[0]}\\nbag :{pr_bag_str[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your tweet:  هذا اللي جانا من ورا اسقاط الولاية مع الأسف\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : هذا اللي جانا من ورا اسقاط الولاية مع الأسف\n",
      "tifidf : POS\n",
      "bag :POS\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter your tweet: \")\n",
    "tr_tifidf_str = tifidf_str[0].transform([text])\n",
    "pr_tifidf_str = tifidf_str[1].predict(tr_tifidf_str)\n",
    "\n",
    "tr_bag_str = bag_str[0].transform([text])\n",
    "pr_bag_str = bag_str[1].predict(tr_bag_str)\n",
    "\n",
    "print(f\"text : {text}\\ntifidf : {pr_tifidf_str[0]}\\nbag :{pr_bag_str[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your tweet:  سعوديات ضد الولاية \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : سعوديات ضد الولاية \n",
      "tifidf : NEG\n",
      "bag :POS\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Enter your tweet: \")\n",
    "tr_tifidf_str = tifidf_str[0].transform([text])\n",
    "pr_tifidf_str = tifidf_str[1].predict(tr_tifidf_str)\n",
    "\n",
    "tr_bag_str = bag_str[0].transform([text])\n",
    "pr_bag_str = bag_str[1].predict(tr_bag_str)\n",
    "\n",
    "print(f\"text : {text}\\ntifidf : {pr_tifidf_str[0]}\\nbag :{pr_bag_str[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
