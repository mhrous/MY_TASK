{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Filtering with linear models\n",
    "In this Exercise you will learn how to predict positive and negative reviews on **movie_reviews** corpus , which categorizes each review as positive or negative.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "In this task you will need the following libraries:\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [NLTK](http://www.nltk.org) — a platform to work with natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "This data is reviews about movies, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"not_spam_sms.txt\", \"r\") as neg_file:\n",
    "    neg_data = [(line, \"neg\") for line in neg_file]\n",
    "with open(\"spam_sms.txt\", \"r\") as pos_file:\n",
    "    pos_data = [(line, \"pos\") for line in pos_file]\n",
    "\n",
    "data = neg_data + pos_data\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "In this section you need to prepare your data.. complete the functions as instructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case_text(in_text):\n",
    "    out_text = in_text.lower()\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace the following symboles by space in text: '[@,;/(){}\\[\\]\\|]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_symbols_text(in_text):\n",
    "    pattern = r\"[@,;/(){}\\[\\]\\|]\"\n",
    "    out_text = re.sub(pattern, \" \", in_text)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove every thing but lower case chars, numbers from 0 to 9 and spaces\n",
    "hint:<br>\n",
    "1- define a regular expression pattern with character class that matches any character other than the ones you want to keep<br>\n",
    "2- replace these symbols with empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(in_text):\n",
    "    pattern = r\"[^\\d\\s\\w]\"\n",
    "    out_text = re.sub(pattern, \"\", in_text)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove stopwords from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords_from_text(in_text):\n",
    "    out_text =\" \".join(w for w in in_text.split() if w not in STOPWORDS)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stem each word in the text from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = lower_case_text(text)\n",
    "    text = rep_symbols_text(text)\n",
    "    text = clean_text(text)\n",
    "    text = remove_stopwords_from_text(text)\n",
    "#     text = stem_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congrats 1 year special cinema pass 2 call 09061209465 c suprman v matrix3 starwars3 etc 4 free bx420ip45we 150pm dont miss'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocess(\"Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"I'm back &amp; we're packing the CAR now, I'll let you know if there's \\\"room\\\"\", \n",
    "                \"I agree that this is the best Michael Myers-based \\\"Halloween\\\" movie since 1981's \\\"Halloween II.\\\"\"]\n",
    "    answers = [\"im back amp packing car ill let know theres room\", \n",
    "               \"agree best michael myersbased halloween movie since 1981s halloween ii\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if text_preprocess(ex) != ans:\n",
    "            print(text_preprocess(ex))\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_text_prepare())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise all reviews in data list, by normalizing the review part of the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "\n",
    "norm_data = [(text_preprocess(t),c) for (t,c) in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_size = int(len(norm_data)*0.70)\n",
    "X = [example[0] for example in norm_data]\n",
    "y = [example[1] for example in norm_data]\n",
    "\n",
    "X_train = X[:training_set_size]\n",
    "y_train = y[:training_set_size]\n",
    "\n",
    "X_test = X[training_set_size:]\n",
    "y_test = y[training_set_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming text to a vector\n",
    "\n",
    "Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "One of the well-known approaches is a *bag-of-words* representation. To create this transformation, You can generate document term matrix by using scikit-learn's CountVectorizer from *scikit-learn*. Use *train* corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you use bigrams along with unigrams in your vocabulary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test — samples        \n",
    "        return Bag of words vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create Count vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "    \n",
    "    ####### YOUR CODE HERE #######\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train_Vec = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_Vec = count_vectorizer.transform(X_test)\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    return X_train_Vec, X_test_Vec, count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bag, X_test_bag, bow_vocab = bow_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3901x7471 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33083 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "The second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space. \n",
    "\n",
    "Implement function *tfidf_features* using class [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from *scikit-learn*. Use *train* corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test — samples        \n",
    "        return TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "    \n",
    "    ####### YOUR CODE HERE #######\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    return X_train, X_test, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "def train_classifier(X_train, y_train):\n",
    "    classifier = RidgeClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_bag = train_classifier(X_train_bag, y_train)\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_labels_bag = classifier_bag.predict(X_test_bag)\n",
    "y_test_predicted_scores_bag = classifier_bag.decision_function(X_test_bag)\n",
    "\n",
    "y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "y_test_predicted_scores_tfidf = classifier_tfidf.decision_function(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:\t500 new mobil 2004 must go txt nokia 89545 collect todayfrom 1 www4tcbiz 2optout 08718726270150gbp m...\n",
      "True label:\tpos\n",
      "Predicted label BoW:\tpos\n",
      "Predicted label Tf_Idf:\tpos\n",
      "\n",
      "\n",
      "Review:\tcant pick phone right pl send messag...\n",
      "True label:\tneg\n",
      "Predicted label BoW:\tneg\n",
      "Predicted label Tf_Idf:\tneg\n",
      "\n",
      "\n",
      "Review:\they look like wrong one kappa guy number still phone want text see he around...\n",
      "True label:\tneg\n",
      "Predicted label BoW:\tneg\n",
      "Predicted label Tf_Idf:\tneg\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('Review:\\t{}\\nTrue label:\\t{}\\nPredicted label BoW:\\t{}\\nPredicted label Tf_Idf:\\t{}\\n\\n'.format(\n",
    "        X_test[i][:100] + \"...\",y_test[i],y_test_predicted_labels_bag[i], y_test_predicted_labels_tfidf[i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words Accuracy: 0.9754931261207412\n",
      "Tfidf Accuracy: 0.9802749551703527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Bag-of-words Accuracy: '+ str(accuracy_score(y_test, y_test_predicted_labels_bag)))\n",
    "print('Tfidf Accuracy: ' + str(accuracy_score(y_test, y_test_predicted_labels_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "Classifier: \t Bag of words \n",
      "Top Possitive Words:\tcontent, claim, 07090201529, ringtone, voicemail\n",
      "Top Negative Words:\tliked, improved, machan, executive, urgnt\n",
      "\n",
      "______________________________\n",
      "Classifier: \t IF-IDF \n",
      "Top Possitive Words:\t150p, servic, mobil, claim, txt\n",
      "Top Negative Words:\tfree call, text your, like new, road, machan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_words_for_tag(classifier, index_to_words):\n",
    "    top_positive_words = [index_to_words[index] for index in classifier.coef_.argsort().tolist()[0][-5:]]  # top-5 words sorted by the coefficiens.\n",
    "    top_negative_words = [index_to_words[index] for index in classifier.coef_.argsort().tolist()[0][:5]] # bottom-5 words  sorted by the coefficients.\n",
    "    print('Top Possitive Words:\\t{}'.format(', '.join(top_positive_words)))\n",
    "    print('Top Negative Words:\\t{}\\n'.format(', '.join(top_negative_words)))\n",
    "\n",
    "print('______________________________\\nClassifier: \\t {} '.format('Bag of words'))\n",
    "print_words_for_tag(classifier_bag, {i:word for word,i in bow_vocab.items()})\n",
    "\n",
    "print('______________________________\\nClassifier: \\t {} '.format('IF-IDF',))\n",
    "print_words_for_tag(classifier_tfidf, {i:word for word,i in tfidf_vocab.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "def stem_text(in_text):\n",
    "    out_text = \" \".join(porter.stem(w) for w in in_text.split())\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_norm_data = [(stem_text(t),c) for (t,c) in norm_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_size = int(len(new_norm_data)*0.70)\n",
    "new_X = [example[0] for example in new_norm_data]\n",
    "new_y = [example[1] for example in new_norm_data]\n",
    "\n",
    "new_X_train = new_X[:training_set_size]\n",
    "new_y_train = new_y[:training_set_size]\n",
    "\n",
    "new_X_test = new_X[training_set_size:]\n",
    "new_y_test = new_y[training_set_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train_bag, new_X_test_bag, new_bow_vocab = bow_features(new_X_train, new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train_tfidf, new_X_test_tfidf, new_tfidf_vocab = tfidf_features(new_X_train, new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_bag = train_classifier(new_X_train_bag, new_y_train)\n",
    "classifier_tfidf = train_classifier(new_X_train_tfidf, new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_test_predicted_labels_bag = classifier_bag.predict(new_X_test_bag)\n",
    "new_y_test_predicted_scores_bag = classifier_bag.decision_function(new_X_test_bag)\n",
    "\n",
    "new_y_test_predicted_labels_tfidf = classifier_tfidf.predict(new_X_test_tfidf)\n",
    "new_y_test_predicted_scores_tfidf = classifier_tfidf.decision_function(new_X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words Accuracy: 0.9731022115959355\n",
      "Tfidf Accuracy: 0.9802749551703527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Bag-of-words Accuracy with stem: '+ str(accuracy_score(new_y_test, new_y_test_predicted_labels_bag)))\n",
    "print('Tfidf Accuracy with stem: ' + str(accuracy_score(new_y_test, new_y_test_predicted_labels_tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
